{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Phase B: Model Diagnostics & Advanced Feature Engineering\n",
    "**Project:** Real Estate Price Prediction (Queretaro, Mexico)\n",
    "**Objective:** Diagnose the econometric anomalies found in the Baseline BigQuery models and implement feature engineering to resolve multicollinearity and endogeneity issues.\n",
    "\n",
    "## 1. Baseline Model \"Post-Mortem\" Analysis\n",
    "In Phase A, we trained two baseline models using BigQuery ML:\n",
    "1.  **Model A (Linear):** Target = `price` ($R^2 \\approx 0.645$)\n",
    "2.  **Model B (Log-Linear):** Target = `ln(price)` ($R^2 \\approx 0.623$)\n",
    "\n",
    "While the $R^2$ results successfully replicated the explanatory power of the reference paper (Guanajuato study), the coefficient analysis revealed three **econometric paradoxes** that must be addressed before model tuning:\n",
    "\n",
    "| model\\_name | feature\\_name | coefficient | standard\\_error | p\\_value | significance\\_level |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Model A \\(Linear\\) | \\_\\_INTERCEPT\\_\\_ | -3569918.99 | 368817.35 | 0 | \\*\\*\\* Highly Significant |\n",
    "| Model A \\(Linear\\) | feat\\_bathrooms | 688079.31 | 50238.23 | 0 | \\*\\*\\* Highly Significant |\n",
    "| Model A \\(Linear\\) | feat\\_bedrooms | -492347.23 | 65832.46 | 0 | \\*\\*\\* Highly Significant |\n",
    "| Model A \\(Linear\\) | feat\\_crime\\_consolidated | 5756.01 | 399.49 | 0 | \\*\\*\\* Highly Significant |\n",
    "| Model A \\(Linear\\) | feat\\_crime\\_homicide | 60931.28 | 33203.45 | 0.066 | \\* Marginally Significant |\n",
    "| Model A \\(Linear\\) | feat\\_dist\\_center | -122.93 | 17.66 | 0 | \\*\\*\\* Highly Significant |\n",
    "| Model A \\(Linear\\) | feat\\_dist\\_park | -244.36 | 44.65 | 0 | \\*\\*\\* Highly Significant |\n",
    "| Model A \\(Linear\\) | feat\\_dist\\_supermarket | 238.07 | 29.54 | 0 | \\*\\*\\* Highly Significant |\n",
    "| Model A \\(Linear\\) | feat\\_has\\_garden | 354276.87 | 101621.16 | 0.0008 | \\*\\*\\* Highly Significant |\n",
    "| Model A \\(Linear\\) | feat\\_m2\\_constructed | 22112.04 | 282.79 | 0 | \\*\\*\\* Highly Significant |\n",
    "| Model A \\(Linear\\) | feat\\_parking\\_spots | 486444.87 | 28637.46 | 0 | \\*\\*\\* Highly Significant |\n",
    "| Model B \\(Log-Linear\\) | \\_\\_INTERCEPT\\_\\_ | 14.0734 | 0.0365 | 0 | \\*\\*\\* Highly Significant |\n",
    "| Model B \\(Log-Linear\\) | feat\\_bathrooms | 0.1811 | 0.0051 | 0 | \\*\\*\\* Highly Significant |\n",
    "| Model B \\(Log-Linear\\) | feat\\_bedrooms | -0.0272 | 0.0066 | 0.0001 | \\*\\*\\* Highly Significant |\n",
    "| Model B \\(Log-Linear\\) | feat\\_crime\\_consolidated | 0.0005 | 0 | 0 | \\*\\*\\* Highly Significant |\n",
    "| Model B \\(Log-Linear\\) | feat\\_crime\\_homicide | -0.0121 | 0.0033 | 0.0004 | \\*\\*\\* Highly Significant |\n",
    "| Model B \\(Log-Linear\\) | feat\\_dist\\_center | 0 | 0 | 0.7211 | Not Significant \\(Likely Noise\\) |\n",
    "| Model B \\(Log-Linear\\) | feat\\_dist\\_park | -0 | 0 | 0.1956 | Not Significant \\(Likely Noise\\) |\n",
    "| Model B \\(Log-Linear\\) | feat\\_dist\\_supermarket | 0 | 0 | 0.0874 | \\* Marginally Significant |\n",
    "| Model B \\(Log-Linear\\) | feat\\_has\\_garden | 0.1832 | 0.0102 | 0 | \\*\\*\\* Highly Significant |\n",
    "| Model B \\(Log-Linear\\) | feat\\_m2\\_constructed | 0.002 | 0 | 0 | \\*\\*\\* Highly Significant |\n",
    "| Model B \\(Log-Linear\\) | feat\\_parking\\_spots | 0.0393 | 0.0029 | 0 | \\*\\*\\* Highly Significant |\n",
    "\n",
    "\n",
    "### A. The \"Negative Bedroom\" Paradox\n",
    "* **Observation:** The coefficient for `feat_bedrooms` was **negative** (-492,347 MXN in Linear model).\n",
    "* **Interpretation:** The model suggests that adding a bedroom *reduces* the house value by half a million pesos, holding other factors constant.\n",
    "* **Diagnosis:** High probability of **Multicollinearity**. Since `feat_m2_constructed` (size) is already in the model, `bedrooms` and `size` are fighting to explain the same variance. A small house with many rooms implies tiny, lower-value rooms.\n",
    "\n",
    "### B. The \"Crime Value\" Paradox\n",
    "* **Observation:** The coefficient for `feat_crime_consolidated` was **positive** (+5,756 MXN).\n",
    "* **Interpretation:** The model implies that higher crime rates are associated with *higher* property values.\n",
    "* **Diagnosis:** **Endogeneity / Omitted Variable Bias**. Crime is likely acting as a proxy for **economic activity**. Wealthy areas attract more opportunistic crime than undeveloped areas. The model is confusing \"wealthy target\" with \"valuable location.\"\n",
    "\n",
    "### C. The \"Vanishing Distance\" Effect\n",
    "* **Observation:** In the Log-Linear Model (Model B), distance variables (Park, Center) became statistically insignificant ($p\\text{-value} > 0.05$).\n",
    "* **Diagnosis:** The logarithmic transformation of the price compressed the variance so much that the subtle linear effect of distance was lost. This suggests the relationship between distance and price might be non-linear or strictly local (e.g., only matters within 500m).\n",
    "\n",
    "---\n",
    "## 2. Hypothesis & Action Plan\n",
    "To resolve these issues and improve $R^2$ > 0.70, we will test the following hypotheses:\n",
    "\n",
    "1. **H1 (Interaction Features):** Creating interaction terms (e.g., `bathrooms_per_bedroom`) will resolve the multicollinearity between size and room counts.\n",
    "\n",
    "2. **H2 (Multicollinearity):** Removing `bedrooms` or creating an interaction term (e.g., `bathrooms_per_bedroom`) will stabilize the coefficients of physical characteristics.\n",
    "3. **H3 (Crime Unbundling):** Instead of a consolidated crime rate, we will separate **Violent Crime** (expected negative impact) from **Property Crime** (proxy for wealth) to isolate the true \"fear factor.\"\n",
    "4. **H4 (Rich Features):** Incorporating specific amenities (Schools, Hospitals) will capture the variance currently attributed to \"noise\" in the Baseline."
   ],
   "id": "a56d1240f110eac0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T05:25:23.640088Z",
     "start_time": "2026-02-18T05:25:23.628569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from google.cloud import bigquery\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Visualization Settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Initialize BigQuery Client\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    client = bigquery.Client()\n",
    "    print(\"Successful connection to BigQuery (Authenticated via Environment Variables)\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection Error: {e}\")\n",
    "    print(\"Tip: Verify that your .env file has the correct path to the JSON.\")\n",
    "\n",
    "print(\"Libraries loaded. Ready for diagnostics.\")"
   ],
   "id": "a83a2b6b40f2c99c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful connection to BigQuery (Authenticated via Environment Variables)\n",
      "Libraries loaded. Ready for diagnostics.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Data Loading: Unleashing the Full OBT (One Big Table)\n",
    "\n",
    "In Phase A (Baseline), we intentionally restricted our analysis to a limited subset of variables to strictly replicate the methodology of the reference paper (Guanajuato study).\n",
    "\n",
    "**Transition to Phase B:**\n",
    "Now that the baseline is established, we proceed to unlock the full potential of our **One Big Table (OBT)**. Unlike the original study, our dataset includes a richer set of granular features collected from multiple sources.\n",
    "\n",
    "**Key Additions in this Phase:**\n",
    "* **Crime Granularity:** Instead of a consolidated sum, we access specific crime types (Homicide, Burglary, Violence, Vehicle Theft) to test the \"Unbundling\" hypothesis.\n",
    "* **Expanded Amenities:** We include previously unused proximity features (Schools, Hospitals, etc.) to capture value drivers that were treated as \"noise\" in the baseline.\n",
    "* **Full Physical Specs:** Detailed breakdown of property characteristics (e.g., Half-bathrooms, Age of construction).\n",
    "\n",
    "We will now query the full `view_training_data_cleaned` to perform the advanced diagnostic."
   ],
   "id": "adf89aac97f63b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T05:25:29.005383Z",
     "start_time": "2026-02-18T05:25:23.688922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Query to fetch all potential features + target\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    -- Listing ID\n",
    "    listing_id,\n",
    "\n",
    "    -- Target\n",
    "    target_price,\n",
    "\n",
    "    -- Physical Features\n",
    "    feat_m2_constructed,\n",
    "    feat_m2_terrain,\n",
    "    feat_bedrooms,\n",
    "    feat_bathrooms,\n",
    "    feat_parking_spots,\n",
    "\n",
    "    -- Amenities\n",
    "    feat_is_new,\n",
    "    feat_has_security,\n",
    "    feat_has_garden,\n",
    "    feat_has_pool,\n",
    "    feat_has_gym,\n",
    "    feat_has_kitchen,\n",
    "    feat_has_terrace,\n",
    "\n",
    "    -- Municipality\n",
    "    feat_municipality,\n",
    "\n",
    "    -- Geographic Features (Distances)\n",
    "    feat_dist_mall,\n",
    "    feat_dist_park,\n",
    "    feat_dist_industrial,\n",
    "    feat_dist_green_area,\n",
    "    feat_dist_playground,\n",
    "    feat_dist_service,\n",
    "    feat_dist_convenience,\n",
    "    feat_dist_market,\n",
    "    feat_dist_supermarket,\n",
    "    feat_dist_center,\n",
    "    feat_dist_tourism,\n",
    "\n",
    "    -- Crime Features (Raw Counts)\n",
    "    feat_crime_residential,\n",
    "    feat_crime_vehicle,\n",
    "    feat_crime_passerby,\n",
    "    feat_crime_homicide,\n",
    "    feat_crime_injueries,\n",
    "    feat_crime_drug_dealing,\n",
    "    feat_crime_violent\n",
    "\n",
    "FROM `real-estate-qro.queretaro_data_marts.obt_listings_valuation_features`\n",
    "WHERE target_price IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "# Load to DataFrame\n",
    "df = client.query(query).to_dataframe()\n",
    "\n",
    "# Quick sanity check\n",
    "print(f\"Dataset Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "df.head()"
   ],
   "id": "90c06fa2d29a5287",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a-b-e\\anaconda3\\envs\\real_estate_ds\\lib\\site-packages\\google\\cloud\\bigquery\\table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: 11840 rows, 33 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                         listing_id  target_price  feat_m2_constructed  \\\n",
       "0  242954b8c44599b049768ae9892a8101   3300000.000            39000.000   \n",
       "1  0202c82d0c6a8a1f08dba979d4917c84   4440000.000              290.000   \n",
       "2  4c83388de8b58047f75e9229b3e27fae   3000000.000              124.000   \n",
       "3  2fb020747d1f874a915cecdb76ce8996   2690000.000              150.000   \n",
       "4  4b5993d3004095aea3cce48a00b43abf   2990000.000              175.000   \n",
       "\n",
       "   feat_m2_terrain  feat_bedrooms  feat_bathrooms  feat_parking_spots  \\\n",
       "0        39000.000          5.000           5.000               5.000   \n",
       "1          212.000          3.000           3.000               2.000   \n",
       "2           90.000          3.000           3.000               1.000   \n",
       "3          115.000          3.000           3.000               2.000   \n",
       "4          115.000          3.000           3.000               2.000   \n",
       "\n",
       "   feat_is_new  feat_has_security  feat_has_garden  ...  \\\n",
       "0            1                  0                0  ...   \n",
       "1            1                  0                1  ...   \n",
       "2            1                  0                1  ...   \n",
       "3            1                  1                1  ...   \n",
       "4            1                  1                1  ...   \n",
       "\n",
       "   feat_dist_supermarket  feat_dist_center  feat_dist_tourism  \\\n",
       "0              16135.467         16371.798           2437.980   \n",
       "1              20000.000          9662.670          12920.501   \n",
       "2              20000.000          1844.340          20000.000   \n",
       "3              17730.578         16905.141           8779.006   \n",
       "4              17730.578         16905.141           8779.006   \n",
       "\n",
       "   feat_crime_residential feat_crime_vehicle  feat_crime_passerby  \\\n",
       "0                  26.621              0.000                3.328   \n",
       "1                   0.000              0.000                0.000   \n",
       "2                   0.000              0.000                0.000   \n",
       "3                  53.115              5.902                0.000   \n",
       "4                  53.115              5.902                0.000   \n",
       "\n",
       "   feat_crime_homicide  feat_crime_injueries  feat_crime_drug_dealing  \\\n",
       "0                9.983               126.452                   29.949   \n",
       "1                0.000                 0.000                    0.000   \n",
       "2                0.000                 0.000                    0.000   \n",
       "3                0.000               132.786                   20.656   \n",
       "4                0.000               132.786                   20.656   \n",
       "\n",
       "   feat_crime_violent  \n",
       "0               0.000  \n",
       "1               0.000  \n",
       "2               0.000  \n",
       "3               2.951  \n",
       "4               2.951  \n",
       "\n",
       "[5 rows x 33 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>target_price</th>\n",
       "      <th>feat_m2_constructed</th>\n",
       "      <th>feat_m2_terrain</th>\n",
       "      <th>feat_bedrooms</th>\n",
       "      <th>feat_bathrooms</th>\n",
       "      <th>feat_parking_spots</th>\n",
       "      <th>feat_is_new</th>\n",
       "      <th>feat_has_security</th>\n",
       "      <th>feat_has_garden</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_dist_supermarket</th>\n",
       "      <th>feat_dist_center</th>\n",
       "      <th>feat_dist_tourism</th>\n",
       "      <th>feat_crime_residential</th>\n",
       "      <th>feat_crime_vehicle</th>\n",
       "      <th>feat_crime_passerby</th>\n",
       "      <th>feat_crime_homicide</th>\n",
       "      <th>feat_crime_injueries</th>\n",
       "      <th>feat_crime_drug_dealing</th>\n",
       "      <th>feat_crime_violent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>242954b8c44599b049768ae9892a8101</td>\n",
       "      <td>3300000.000</td>\n",
       "      <td>39000.000</td>\n",
       "      <td>39000.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16135.467</td>\n",
       "      <td>16371.798</td>\n",
       "      <td>2437.980</td>\n",
       "      <td>26.621</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.328</td>\n",
       "      <td>9.983</td>\n",
       "      <td>126.452</td>\n",
       "      <td>29.949</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0202c82d0c6a8a1f08dba979d4917c84</td>\n",
       "      <td>4440000.000</td>\n",
       "      <td>290.000</td>\n",
       "      <td>212.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>20000.000</td>\n",
       "      <td>9662.670</td>\n",
       "      <td>12920.501</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4c83388de8b58047f75e9229b3e27fae</td>\n",
       "      <td>3000000.000</td>\n",
       "      <td>124.000</td>\n",
       "      <td>90.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>20000.000</td>\n",
       "      <td>1844.340</td>\n",
       "      <td>20000.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2fb020747d1f874a915cecdb76ce8996</td>\n",
       "      <td>2690000.000</td>\n",
       "      <td>150.000</td>\n",
       "      <td>115.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>17730.578</td>\n",
       "      <td>16905.141</td>\n",
       "      <td>8779.006</td>\n",
       "      <td>53.115</td>\n",
       "      <td>5.902</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>132.786</td>\n",
       "      <td>20.656</td>\n",
       "      <td>2.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4b5993d3004095aea3cce48a00b43abf</td>\n",
       "      <td>2990000.000</td>\n",
       "      <td>175.000</td>\n",
       "      <td>115.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>17730.578</td>\n",
       "      <td>16905.141</td>\n",
       "      <td>8779.006</td>\n",
       "      <td>53.115</td>\n",
       "      <td>5.902</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>132.786</td>\n",
       "      <td>20.656</td>\n",
       "      <td>2.951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Data Cleansing & Understanding\n",
    "**Objective:** Transform raw OBT data into a reliable, logic-consistent dataset.\n",
    "Instead of treating validation and cleaning as separate phases, we apply a **\"Detect & Fix\"** strategy per variable group.\n",
    "\n",
    "**Methodology per Group:**\n",
    "1.  **Structural Integrity:** Checking for duplicates, unexpected NaNs, and data types.\n",
    "2.  **Business Logic:** Enforcing real-world real estate rules (e.g., *Construction Area* cannot exceed *Terrain Area* in single-family homes).\n",
    "3.  **Statistical Cleaning:** Handling extreme outliers and rare categories that generate noise.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1. Global Health Check\n",
    "Before diving into specific columns, we assess the dataset's overall health.\n",
    "* **Action:** Remove full-row duplicates.\n",
    "* **Action:** Drop columns with >50% missing data (unless specifically imputable)."
   ],
   "id": "1383b2563e0f5c4d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T05:25:38.765944Z",
     "start_time": "2026-02-18T05:25:38.737002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Full-Row Duplicates Removal ---\n",
    "\n",
    "# 1. Initial Snapshot\n",
    "initial_rows = df.shape[0]\n",
    "print(f\"Initial Row Count: {initial_rows:,}\")\n",
    "\n",
    "# 2. Identify and Remove Exact Duplicates (Full Row)\n",
    "# We look for rows where all columns are identical\n",
    "duplicates_mask = df.duplicated(keep='first')\n",
    "n_exact_dupes = duplicates_mask.sum()\n",
    "\n",
    "if n_exact_dupes > 0:\n",
    "    # We remove duplicates and reset the index to keep the dataframe clean\n",
    "    df = df[~duplicates_mask].reset_index(drop=True)\n",
    "    print(f\"Removed {n_exact_dupes:,} exact duplicate rows ({(n_exact_dupes/initial_rows):.2%})\")\n",
    "else:\n",
    "    print(\"No exact duplicates found. SQL logic worked perfectly.\")\n",
    "\n",
    "# 3. ID Integrity Verification (Business Key Check)\n",
    "# Sometimes the ID repeats but the content varies (e.g., price updates creating new rows)\n",
    "if 'listing_id' in df.columns:\n",
    "    # Check for duplicates specifically in the Primary Key\n",
    "    id_dupes = df.duplicated(subset=['listing_id'], keep='first').sum()\n",
    "\n",
    "    if id_dupes > 0:\n",
    "        print(f\"WARNING: Found {id_dupes:,} listings with duplicated IDs but different content.\")\n",
    "        print(\"    -> Taking action: Keeping only the first occurrence per ID to ensure uniqueness.\")\n",
    "\n",
    "        # Corrective Action: Keep the first occurrence (assuming chronological load order)\n",
    "        df = df.drop_duplicates(subset=['listing_id'], keep='first').reset_index(drop=True)\n",
    "        print(f\"    -> Removed {id_dupes:,} ID duplicates.\")\n",
    "    else:\n",
    "        print(\"Primary Key integrity confirmed (All 'listing_id' are unique).\")\n",
    "\n",
    "# 4. Final Report\n",
    "final_rows = df.shape[0]\n",
    "data_loss = initial_rows - final_rows\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Final Row Count:    {final_rows:,}\")\n",
    "print(f\"Total Data Dropped: {data_loss:,} rows ({(data_loss/initial_rows):.2%})\")\n",
    "print(\"-\" * 40)"
   ],
   "id": "adc4e41368a5a17b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Row Count: 11,840\n",
      "No exact duplicates found.\n",
      "Primary Key integrity confirmed (All 'listing_id' are unique).\n",
      "----------------------------------------\n",
      "Final Row Count:    11,840\n",
      "Total Data Dropped: 0 rows (0.00%)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T05:35:13.729022Z",
     "start_time": "2026-02-18T05:35:13.711660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- MISSING VALUES ANALYSIS & FILTERING ---\n",
    "\n",
    "# 1. Calculate Missing Percentage per Column\n",
    "missing_series = df.isnull().mean()\n",
    "missing_cols = missing_series[missing_series > 0]\n",
    "\n",
    "# 2. Reporting\n",
    "if not missing_cols.empty:\n",
    "    print(f\"Found {len(missing_cols)} columns with missing values:\")\n",
    "\n",
    "    # Create a temporary DataFrame for a cleaner report\n",
    "    report = pd.DataFrame({\n",
    "        'Missing %': missing_series * 100,\n",
    "        'Count': df.isnull().sum()\n",
    "    })\n",
    "    # Display only columns with missing values, sorted descending\n",
    "    # Note: 'display()' works in Jupyter Notebooks. Use 'print()' for standard scripts.\n",
    "    display(report[report['Count'] > 0].sort_values(by='Missing %', ascending=False))\n",
    "else:\n",
    "    print(\"No missing values found. SQL `COALESCE` logic worked perfectly.\")\n",
    "\n",
    "# 3. Apply the 50% Threshold Rule (Drop Columns)\n",
    "# If a column is more than 50% empty, it provides insufficient signal for the model.\n",
    "THRESHOLD = 0.50\n",
    "cols_to_drop = missing_series[missing_series > THRESHOLD].index.tolist()\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"\\nDROPPING {len(cols_to_drop)} columns with >{THRESHOLD:.0%} missing data:\")\n",
    "    print(f\"   -> Columns: {cols_to_drop}\")\n",
    "\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    print(\"   -> Action completed successfully.\")\n",
    "else:\n",
    "    print(f\"\\nNo columns exceeded the {THRESHOLD:.0%} missing threshold.\")\n",
    "\n",
    "# 4. Final Sanity Check (Residuals)\n",
    "# Check if any small amount of NaNs remain (e.g., <50%) to be handled later.\n",
    "remaining_nans = df.isnull().sum().sum()\n",
    "\n",
    "if remaining_nans > 0:\n",
    "    print(f\"\\nNote: There are still {remaining_nans} missing values scattered across the dataset.\")\n",
    "    print(\"   -> These must be handled in the specific Imputation Step (if applicable).\")\n",
    "else:\n",
    "    print(\"\\nDataset is 100% complete (No NaNs). Ready for the next step.\")"
   ],
   "id": "c07e22d44b3a2669",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excellent! No missing values found. SQL `COALESCE` logic worked perfectly.\n",
      "\n",
      "No columns exceeded the 50% missing threshold.\n",
      "\n",
      "Dataset is 100% complete (No NaNs). Ready for the next step.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2. Target Variable Validation (`price` & `m2_price`)\n",
    "The model cannot learn from listings without a valid price. This is our \"Hard Filter\".\n",
    "* **Logic Check:** Remove `price <= 0`.\n",
    "* **Outlier Check:** Identify and remove unrealistic prices (e.g., $\\$$100 MXN or $\\$$100 Billion MXN) using IQR or hard caps based on local market knowledge."
   ],
   "id": "3b98cef2fc2d0bca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.3. Geographic & Categorical Cleaning (`municipality`, `type`)\n",
    "Addressing cardinality and representation issues.\n",
    "* **Imbalance Check:** Analyze the distribution of listings per municipality.\n",
    "* **Fix:** Apply **\"Rare Label Encoding\"**. Municipalities with insufficient data (e.g., $N < 100$) will be grouped into an `OTHER` category to maintain model stability."
   ],
   "id": "2c244828cd0dfa6b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.4. Discrete Variables (`bedrooms`, `bathrooms`, `parking`)\n",
    "* **Zero vs. NaN:** validate if `0 bedrooms` represents a \"Studio/Loft\" or missing data.\n",
    "* **Business Logic:** Flag listings with 10+ bathrooms but only 2 bedrooms (likely data entry errors)."
   ],
   "id": "5896900d35823609"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.5. Continuous & Spatial Validation (`m2`, `distances`)\n",
    "* **Logic Check (The \"Impossible House\"):** Ensure `m2_constructed <= m2_terrain` (where applicable).\n",
    "* **Distribution Check:** Analyze the \"Distance Spike\" (centroid clustering) identified in previous exploration. We acknowledge this as a data source characteristic, not an error to be deleted."
   ],
   "id": "69af3490a3fae65a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T05:27:11.280209Z",
     "start_time": "2026-02-18T05:27:11.238667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- DATA INTEGRITY REPORT ---\n",
    "\n",
    "def check_integrity(df):\n",
    "    \"\"\"\n",
    "    Generates a summary report of the dataframe's health.\n",
    "    \"\"\"\n",
    "\n",
    "    # Nulls & Zeros Analysis\n",
    "    integrity_df = pd.DataFrame({\n",
    "        'Dtype': df.dtypes,\n",
    "        'Nulls': df.isnull().sum(),\n",
    "        'Null_Pct': (df.isnull().sum() / len(df)) * 100,\n",
    "        'Zeros': (df == 0).sum(),\n",
    "        'Zero_Pct': ((df == 0).sum() / len(df)) * 100,\n",
    "        'Skewness': df.select_dtypes(include=[np.number]).skew()\n",
    "    })\n",
    "\n",
    "    # Filter only relevant columns (exclude IDs or non-numeric if needed)\n",
    "    return integrity_df\n",
    "\n",
    "# Execute the check\n",
    "integrity_report = check_integrity(df)\n",
    "\n",
    "# Display the report with highlighting for potential issues\n",
    "# - High Null % -> Candidates for dropping or imputation\n",
    "# - High Skewness (> 1 or < -1) -> Candidates for Log Transformation (H4)\n",
    "# - Unexpected Zeros -> Potential data quality issues\n",
    "print(\"\\n--- Integrity Report  ---\")\n",
    "display(integrity_report)\n",
    "\n",
    "# --- LOGIC CHECKS (Domain Specific) ---\n",
    "print(\"\\n--- Domain Logic Validation ---\")\n",
    "\n",
    "# Check 1: Houses with 0 Bedrooms, 0 Bathrooms, 0 construction, or 0 land\n",
    "zero_beds = df[df['feat_bedrooms'] == 0].shape[0]\n",
    "zero_baths = df[df['feat_bathrooms'] == 0].shape[0]\n",
    "zero_construction = df[df['feat_m2_constructed'] == 0].shape[0]\n",
    "zero_land = df[df['feat_m2_terrain'] == 0].shape[0]\n",
    "print(f\"Suspicious Records: 0 Bedrooms = {zero_beds}, 0 Bathrooms = {zero_baths}, 0 Construction = {zero_construction}, 0 Land = {zero_land}\")\n",
    "\n",
    "# Check 2: Construction > Land (Valid for apartments, suspicious for houses if we had that flag)\n",
    "# For now, we just check the ratio\n",
    "denser_than_land = df[df['feat_m2_constructed'] > df['feat_m2_terrain']].shape[0]\n",
    "print(f\"Properties with Construction > Land Area: {denser_than_land} (Normal for vertical housing)\")\n",
    "\n",
    "# Check 3: Price Outliers (Simple Min/Max check)\n",
    "print(f\"Price Range: ${df['target_price'].min():,.0f} - ${df['target_price'].max():,.0f}\")"
   ],
   "id": "6f11e9fb0d7e11a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Integrity Report  ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                           Dtype  Nulls  Null_Pct  Zeros  Zero_Pct  Skewness\n",
       "feat_bathrooms           float64      0     0.000    363     3.066     1.615\n",
       "feat_bedrooms            float64      0     0.000    331     2.796    29.497\n",
       "feat_crime_drug_dealing  float64      0     0.000      2     0.017    -0.680\n",
       "feat_crime_homicide      float64      0     0.000     94     0.794    -0.901\n",
       "feat_crime_injueries     float64      0     0.000      2     0.017    -1.899\n",
       "feat_crime_passerby      float64      0     0.000     67     0.566    -0.096\n",
       "feat_crime_residential   float64      0     0.000      2     0.017    -0.079\n",
       "feat_crime_vehicle       float64      0     0.000      3     0.025    -0.759\n",
       "feat_crime_violent       float64      0     0.000     61     0.515    -1.007\n",
       "feat_dist_center         float64      0     0.000     42     0.355     0.066\n",
       "feat_dist_convenience    float64      0     0.000     18     0.152     4.237\n",
       "feat_dist_green_area     float64      0     0.000      0     0.000     1.384\n",
       "feat_dist_industrial     float64      0     0.000      0     0.000     2.001\n",
       "feat_dist_mall           float64      0     0.000     63     0.532     1.310\n",
       "feat_dist_market         float64      0     0.000      7     0.059     0.819\n",
       "feat_dist_park           float64      0     0.000      6     0.051     1.477\n",
       "feat_dist_playground     float64      0     0.000      0     0.000     0.888\n",
       "feat_dist_service        float64      0     0.000      0     0.000     1.268\n",
       "feat_dist_supermarket    float64      0     0.000      0     0.000     1.478\n",
       "feat_dist_tourism        float64      0     0.000      0     0.000     0.571\n",
       "feat_has_garden            Int64      0     0.000   2600    21.959    -1.355\n",
       "feat_has_gym               Int64      0     0.000   8938    75.490     1.185\n",
       "feat_has_kitchen           Int64      0     0.000   1909    16.123    -1.843\n",
       "feat_has_pool              Int64      0     0.000   4648    39.257    -0.440\n",
       "feat_has_security          Int64      0     0.000   3765    31.799    -0.782\n",
       "feat_has_terrace           Int64      0     0.000   3340    28.209    -0.969\n",
       "feat_is_new                Int64      0     0.000   4965    41.934    -0.327\n",
       "feat_m2_constructed      float64      0     0.000    447     3.775    50.525\n",
       "feat_m2_terrain          float64      0     0.000    309     2.610   108.808\n",
       "feat_municipality         object      0     0.000      0     0.000      <NA>\n",
       "feat_parking_spots       float64      0     0.000   1089     9.198     7.332\n",
       "listing_id                object      0     0.000      0     0.000      <NA>\n",
       "target_price             float64      0     0.000      0     0.000   107.384"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dtype</th>\n",
       "      <th>Nulls</th>\n",
       "      <th>Null_Pct</th>\n",
       "      <th>Zeros</th>\n",
       "      <th>Zero_Pct</th>\n",
       "      <th>Skewness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feat_bathrooms</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>363</td>\n",
       "      <td>3.066</td>\n",
       "      <td>1.615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_bedrooms</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>331</td>\n",
       "      <td>2.796</td>\n",
       "      <td>29.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_crime_drug_dealing</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_crime_homicide</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>94</td>\n",
       "      <td>0.794</td>\n",
       "      <td>-0.901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_crime_injueries</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-1.899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_crime_passerby</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>67</td>\n",
       "      <td>0.566</td>\n",
       "      <td>-0.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_crime_residential</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_crime_vehicle</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_crime_violent</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>61</td>\n",
       "      <td>0.515</td>\n",
       "      <td>-1.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_dist_center</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_dist_convenience</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>18</td>\n",
       "      <td>0.152</td>\n",
       "      <td>4.237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_dist_green_area</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_dist_industrial</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_dist_mall</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>63</td>\n",
       "      <td>0.532</td>\n",
       "      <td>1.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_dist_market</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_dist_park</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.051</td>\n",
       "      <td>1.477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_dist_playground</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_dist_service</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_dist_supermarket</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_dist_tourism</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_has_garden</th>\n",
       "      <td>Int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2600</td>\n",
       "      <td>21.959</td>\n",
       "      <td>-1.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_has_gym</th>\n",
       "      <td>Int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8938</td>\n",
       "      <td>75.490</td>\n",
       "      <td>1.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_has_kitchen</th>\n",
       "      <td>Int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1909</td>\n",
       "      <td>16.123</td>\n",
       "      <td>-1.843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_has_pool</th>\n",
       "      <td>Int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4648</td>\n",
       "      <td>39.257</td>\n",
       "      <td>-0.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_has_security</th>\n",
       "      <td>Int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3765</td>\n",
       "      <td>31.799</td>\n",
       "      <td>-0.782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_has_terrace</th>\n",
       "      <td>Int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3340</td>\n",
       "      <td>28.209</td>\n",
       "      <td>-0.969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_is_new</th>\n",
       "      <td>Int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4965</td>\n",
       "      <td>41.934</td>\n",
       "      <td>-0.327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_m2_constructed</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>447</td>\n",
       "      <td>3.775</td>\n",
       "      <td>50.525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_m2_terrain</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>309</td>\n",
       "      <td>2.610</td>\n",
       "      <td>108.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_municipality</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_parking_spots</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1089</td>\n",
       "      <td>9.198</td>\n",
       "      <td>7.332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>listing_id</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_price</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>107.384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Domain Logic Validation ---\n",
      "Suspicious Records: 0 Bedrooms = 331, 0 Bathrooms = 363, 0 Construction = 447, 0 Land = 309\n",
      "Properties with Construction > Land Area: 6324 (Normal for vertical housing)\n",
      "Price Range: $150,000 - $9,700,000,000\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.1. Categorical Imbalance Check: Municipalities\n",
    "Before encoding categorical variables, we assess the frequency distribution of municipalities. Categories with extremely low counts (e.g., < 30 listings) can lead to overfitting or unstable coefficients in Linear Regression."
   ],
   "id": "4bbaf4f444ad00dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- MUNICIPALITY DISTRIBUTION CHECK (Pre-Encoding) ---\n",
    "\n",
    "# Calculate frequencies directly from the text column\n",
    "muni_counts = df['feat_municipality'].value_counts()\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, len(muni_counts) * 0.5))\n",
    "ax = sns.barplot(x=muni_counts.values, y=muni_counts.index, hue=muni_counts.index, palette='viridis', legend=False)\n",
    "\n",
    "plt.title('Distribution of Listings per Municipality')\n",
    "plt.xlabel('Number of Listings')\n",
    "plt.ylabel('Municipality')\n",
    "\n",
    "threshold = 50\n",
    "plt.axvline(x=threshold, color='red', linestyle='--', linewidth=1.5)\n",
    "plt.text(threshold + 5, len(muni_counts)-1, f'Risk Threshold (n<{threshold})', color='red', va='bottom')\n",
    "\n",
    "# Show values in bars for clarification\n",
    "for i, v in enumerate(muni_counts.values):\n",
    "    ax.text(v + 10, i, str(v), color='black', va='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"Municipalities with risk of low representation:\")\n",
    "print(muni_counts[muni_counts < threshold])"
   ],
   "id": "1028a4a1d31b99d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Data Cleaning & Logic Filtering\n",
    "The Integrity Report revealed critical issues that jeopardize model stability:\n",
    "1.  **\"Ghost\" Properties:** Records with 0 bedrooms, 0 bathrooms, 0 land, or 0 constructed square meters. These correspond to **Land/Lots**, not habitable housing. We will filter them out to focus the model on Residential Real Estate (Houses/Apartments).\n",
    "2.  **Extreme Skewness:** Variables like `m2_terrain` (Skew: 108) and `target_price` (Skew: 107) indicate massive outliers (likely industrial plots or data entry errors).\n",
    "3.  **Zero Distances:** While plausible in mixed-use developments, accurate distance is key.\n",
    "\n",
    "**Filtering Rules Applied:**\n",
    "* `target_price`: Keep between $500k and $15M MXN (Focus on residential market, removing luxury mansions and errors).\n",
    "* `m2_constructed`: Must be > 30 m² (removing lots and tiny errors).\n",
    "* `m2_terrain`: Must be < 2,000 m² (removing ranches and industrial lots).\n",
    "* `bedrooms` & `bathrooms`: Must be >= 1 (Must be habitable)."
   ],
   "id": "a420d953ad4ec633"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.1. Stage A: Logical Filtering & Data Formatting\n",
    "Before analyzing distributions, we must ensure the data is logically sound for a residential model and formatted correctly for statistical analysis.\n",
    "\n",
    "**1. Row Filtering (Sanity Check):**\n",
    "* Any record with **0 Bedrooms, 0 Bathrooms, or 0 Constructed $m^2$** is classified as a \"Lot/Land\" or data entry error. These are removed.\n",
    "\n",
    "**2. Value Capping (Outlier Control):**\n",
    "* Discrete variables like Bedrooms and Parking often contain extreme outliers (e.g., \"30 parking spots\" or \"15 bedrooms\") which are likely data entry errors or non-residential properties (hotels/offices).\n",
    "* We apply a **Hard Cap** (e.g., max 10) to keep these variables within a realistic residential range.\n",
    "\n",
    "**3. Categorical Encoding:**\n",
    "* The `feat_municipality` variable is text-based. We convert it to numerical format using **One-Hot Encoding** (creating dummy variables) so it can be included in correlation and regression analyses."
   ],
   "id": "c128170b7d6881fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- STAGE A: LOGICAL FILTERS & FORMATTING ---\n",
    "\n",
    "print(f\"Original Count: {df.shape[0]}\")\n",
    "\n",
    "# 1. ROW FILTERING: Identify rows with Zeros\n",
    "# We assume a valid house must have at least 1 bedroom, 1 bathroom, and > 10m2 construction\n",
    "mask_habitable = (df['feat_bedrooms'] > 0) & \\\n",
    "                 (df['feat_bathrooms'] > 0) & \\\n",
    "                 (df['feat_m2_constructed'] > 10)\n",
    "\n",
    "df_logic = df[mask_habitable].copy()\n",
    "\n",
    "dropped = df.shape[0] - df_logic.shape[0]\n",
    "print(f\"1. Row Filtering: Dropped {dropped} records (Land/Lots or Errors)\")\n",
    "\n",
    "# 2. VALUE CAPPING (Clip outliers)\n",
    "# Forces values > 10 to become 10. This handles errors like \"20 bedrooms\".\n",
    "cols_to_cap = ['feat_bedrooms', 'feat_bathrooms', 'feat_parking_spots']\n",
    "cap_limit = 10\n",
    "\n",
    "print(f\"2. Value Capping (Max {cap_limit}):\")\n",
    "for col in cols_to_cap:\n",
    "    if col in df_logic.columns:\n",
    "        # Check how many were capped\n",
    "        n_capped = (df_logic[col] > cap_limit).sum()\n",
    "        df_logic[col] = df_logic[col].clip(upper=cap_limit)\n",
    "        print(f\"   - {col}: Capped {n_capped} values > {cap_limit}\")\n",
    "\n",
    "# 3. CATEGORICAL ENCODING (Municipality)\n",
    "if 'feat_municipality' in df_logic.columns:\n",
    "    print(f\"3. Categorical Encoding: One-Hot Encoding applied to 'feat_municipality'\")\n",
    "    # drop_first=True avoids the Dummy Variable Trap (Multicollinearity)\n",
    "    df_logic = pd.get_dummies(df_logic, columns=['feat_municipality'], prefix='muni', drop_first=True)\n",
    "else:\n",
    "    print(\"3. Categorical Encoding: 'feat_municipality' not found (skipped)\")\n",
    "\n",
    "# Update main df\n",
    "df = df_logic.copy()\n",
    "\n",
    "print(f\"\\n--- STAGE A COMPLETE ---\")\n",
    "print(f\"Final Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ],
   "id": "176f5ed274167e57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- SPIKE DIAGNOSIS: THE CENTROID HYPOTHESIS ---\n",
    "\n",
    "# 1. Identificar el valor exacto del pico (Moda)\n",
    "spike_distance = df['feat_dist_mall'].mode()[0]\n",
    "\n",
    "# 2. Filtrar las casas que tienen esa distancia EXACTA\n",
    "spike_df = df[df['feat_dist_mall'] == spike_distance]\n",
    "\n",
    "print(f\"Distancia del Pico: {spike_distance:.4f} metros\")\n",
    "print(f\"Cantidad de casas en el pico: {len(spike_df)}\")\n",
    "\n",
    "# 3. LA PRUEBA DE FUEGO: ¿Tienen coordenadas únicas o repetidas?\n",
    "# (Asumiendo que tienes lat/lon o 'listing_geom' en el df original, si no, usa el ID del barrio si lo tienes)\n",
    "# Si no tienes lat/lon a la mano, omite este paso, pero es la clave.\n",
    "if 'feat_latitude' in df.columns:\n",
    "    unique_locations = spike_df[['feat_latitude', 'feat_longitude']].drop_duplicates().shape[0]\n",
    "    print(f\"Ubicaciones únicas en el pico: {unique_locations}\")\n",
    "\n",
    "    if unique_locations < 5:\n",
    "        print(\"🚨 DIAGNÓSTICO: CLUSTER DE CENTROIDES DETECTADO.\")\n",
    "        print(\"Todas estas casas comparten la misma ubicación geográfica (probablemente el centro de una colonia).\")\n",
    "    else:\n",
    "        print(\"✅ DIAGNÓSTICO: Es un desarrollo grande real, las casas tienen ubicaciones distintas.\")"
   ],
   "id": "a7b574f0f683a217",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.2. Phase B: Variable Segmentation & Distribution Analysis\n",
    "With the data logically cleaned and formatted in Stage A, we now focus on statistical transformations. We categorize features into four strategic groups to apply specific treatments:\n",
    "\n",
    "1.  **Group A (Heavyweights):** Continuous variables with high variance and potential industrial-scale outliers (Price, m² Construction, m² Land).\n",
    "2.  **Group B (Decay & Counts):** Variables that typically follow a Power Law or have diminishing returns (Distances, Crime Rates).\n",
    "3.  **Group C (Discrete):** Low-cardinality counters (Bedrooms, Bathrooms, Parking) - *Already capped and cleaned in Stage A*.\n",
    "4.  **Group D (Booleans):** Binary indicators (Amenities) and One-Hot Encoded features (Municipalities).\n",
    "\n",
    "**Strategy:**\n",
    "* **Group A:** **Domain Hard Caps** (Removing non-residential outliers like $>\\$80M$) + **Log Transformation** + **Z-Score Pruning** (Statistical cleaning).\n",
    "* **Group B:** **Skewness Check** + **Log Transformation** (Applied if Skew > 1.0 to linearize relationships).\n",
    "* **Group C:** **Distribution Visualization** (Verifying the effectiveness of Stage A capping).\n",
    "* **Group D:** **Variance Threshold** (Dropping features with near-zero variance, e.g., >99.5% constant)."
   ],
   "id": "8b98215452de754c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- STEP 1: AUTOMATED VARIABLE SEGMENTATION ---\n",
    "\n",
    "# 1. Initialize lists\n",
    "group_a_heavyweights = ['target_price', 'feat_m2_constructed', 'feat_m2_terrain']\n",
    "group_b_decay = []\n",
    "group_c_counters = []\n",
    "group_d_booleans = []\n",
    "orphans = []\n",
    "\n",
    "# 2. Automated sorting logic\n",
    "for col in df.columns:\n",
    "    # SKIP: If it's already in Group A, skip to avoid duplication\n",
    "    if col in group_a_heavyweights:\n",
    "        continue\n",
    "\n",
    "    # GROUP B: Decay/Skewed (Distances & Crime)\n",
    "    # Priority: Check prefixes first so 'crime' doesn't fall into discrete counters\n",
    "    if col.startswith('feat_dist_') or col.startswith('feat_crime_'):\n",
    "        group_b_decay.append(col)\n",
    "\n",
    "    # GROUP D: Booleans (0 or 1)\n",
    "    # This now automatically captures the new 'muni_X' columns form Stage A\n",
    "    elif df[col].nunique() == 2:\n",
    "        group_d_booleans.append(col)\n",
    "\n",
    "    # GROUP C: Discrete Counters (Bedrooms, Bathrooms, Parking)\n",
    "    # Logic: Numeric type AND low cardinality (thanks to Stage A capping, this is safe)\n",
    "    elif df[col].dtype in ['int64', 'float64'] and df[col].nunique() < 25:\n",
    "        group_c_counters.append(col)\n",
    "\n",
    "    # ORPHANS: Catch any variable that didn't fit (debugging)\n",
    "    else:\n",
    "        orphans.append(col)\n",
    "\n",
    "# 3. Report\n",
    "print(f\"--- SEGMENTATION REPORT ---\")\n",
    "print(f\"Group A (Heavyweights): {len(group_a_heavyweights)} variables -> {group_a_heavyweights}\")\n",
    "print(f\"Group B (Decay/Skewed): {len(group_b_decay)} variables (e.g., {group_b_decay[:2]}...)\")\n",
    "print(f\"Group C (Discrete):     {len(group_c_counters)} variables -> {group_c_counters}\")\n",
    "print(f\"Group D (Booleans):     {len(group_d_booleans)} variables (Includes amenities + municipalities)\")\n",
    "\n",
    "if orphans:\n",
    "    print(f\"\\nWARNING: {len(orphans)} Orphan variables found (Check manually): {orphans}\")\n",
    "else:\n",
    "    print(\"\\nAll variables successfully segmented.\")"
   ],
   "id": "5b5f3cb63c957293",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 5.2.1. Group A Analysis: Heavyweights (Price & Size)\n",
    "These variables define the property's core value. Even after Stage A cleaning, they may contain **Domain Outliers** (e.g., Ultra-luxury mansions or Industrial properties) that skew the model.\n",
    "\n",
    "**Strategy:**\n",
    "1.  **Domain Hard Caps:** We apply a final filter to remove properties that, while technically valid, are outside the scope of a standard residential model (e.g., Price > $80M MXN).\n",
    "2.  **Log-Transformation:** We apply `np.log1p` to normalize the naturally right-skewed distribution of real estate prices.\n",
    "3.  **Z-Score Pruning:** We remove statistical outliers (> 3 Standard Deviations) in the log-space to ensure a robust regression."
   ],
   "id": "a48cd3e3aba144f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy import stats\n",
    "\n",
    "# --- GROUP A: DOMAIN & STATISTICAL CLEANING ---\n",
    "print(f\"Rows before Group A cleaning: {df.shape[0]}\")\n",
    "\n",
    "# 1. DOMAIN HARD CAPS (The \"Scope\" Filter)\n",
    "# Removing ultra-luxury or industrial outliers\n",
    "mask_domain = (df['target_price'] <= 80_000_000) & \\\n",
    "              (df['feat_m2_constructed'] <= 3_000) & \\\n",
    "              (df['feat_m2_terrain'] <= 10_000)\n",
    "\n",
    "df = df[mask_domain].copy()\n",
    "print(f\"Rows after Domain Caps:       {df.shape[0]} (Removed Industrial/Ultra-Luxury)\")\n",
    "\n",
    "# 2. LOG TRANSFORMATION & Z-SCORE\n",
    "# We work on the log-scale for statistical stability\n",
    "for col in group_a_heavyweights:\n",
    "    # Create Log column (e.g., ln_price)\n",
    "    log_col = f\"ln_{col.replace('feat_', '').replace('target_', '')}\"\n",
    "    df[log_col] = np.log1p(df[col])\n",
    "\n",
    "    # Calculate Z-Score on the new Log column\n",
    "    z_scores = np.abs(stats.zscore(df[log_col]))\n",
    "\n",
    "    # Filter Statistical Outliers (> 3 Sigma)\n",
    "    df = df[z_scores < 3]\n",
    "\n",
    "print(f\"Rows after Z-Score Filter:    {df.shape[0]} (Final Clean Count)\")"
   ],
   "id": "663948e0f21ddad5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. VISUAL CHECK (Log-Space)\n",
    "\n",
    "# Vamos a comparar solo el PRECIO para demostrar el impacto de la transformación\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Original Price\n",
    "original_price = np.expm1(df['ln_price'])\n",
    "\n",
    "sns.histplot(original_price, ax=axes[0], color='gray')\n",
    "axes[0].set_title('Original Price Distribution (Skewed)')\n",
    "axes[0].set_xlabel('Price (MXN)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Log Price\n",
    "sns.histplot(df['ln_price'], ax=axes[1], color='green', kde=True)\n",
    "axes[1].set_title('Log-Transformed Price (Normal)')\n",
    "axes[1].set_xlabel('Log Price')\n",
    "\n",
    "# QQ Plot\n",
    "stats.probplot(df['ln_price'], dist=\"norm\", plot=axes[2])\n",
    "axes[2].set_title('Q-Q Plot: Normality Check')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "b352340acfb05096",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 5.2.2. Group B Analysis: Decay Factors (Distance & Crime)\n",
    "Variables like \"Distance to Park\" or \"Crime Rate\" typically exhibit **Diminishing Marginal Utility** (or Disutility).\n",
    "* *Example:* The price difference between 0km and 1km is huge; between 20km and 21km is negligible.\n",
    "\n",
    "**Strategy:**\n",
    "1.  **Skewness Check:** Identify variables with a long tail (Skew > 1.0).\n",
    "2.  **Log-Transformation:** Apply `log(x+1)` to linearize these relationships for the regression model."
   ],
   "id": "1f74f07e24a2962"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- GROUP B: SKEWNESS & LOG TRANSFORMATION ---\n",
    "\n",
    "skewed_feats = []\n",
    "print(f\"Analyzing {len(group_b_decay)} variables for skewness...\")\n",
    "\n",
    "for col in group_b_decay:\n",
    "    skew = df[col].skew()\n",
    "\n",
    "    # Apply Log if Skewness > 1.0 (Right-skewed)\n",
    "    if skew > 1.0:\n",
    "        new_col = f\"ln_{col.replace('feat_', '')}\"\n",
    "        df[new_col] = np.log1p(df[col])\n",
    "        skewed_feats.append((col, skew))\n",
    "\n",
    "print(f\"\\nTransformed {len(skewed_feats)} variables (Skew > 1.0).\")\n",
    "# Show top 3 examples\n",
    "for original, val in skewed_feats[:3]:\n",
    "    print(f\"   - {original}: Skew={val:.2f} -> created 'ln_{original.replace('feat_', '')}'\")\n",
    "\n",
    "# Visual Proof (Before vs After)\n",
    "if skewed_feats:\n",
    "    example_col = skewed_feats[0][0]\n",
    "    example_log = f\"ln_{example_col.replace('feat_', '')}\"\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    sns.histplot(df[example_col], ax=ax[0], color='gray', bins=30).set_title(f'Original: {example_col}')\n",
    "    sns.histplot(df[example_log], ax=ax[1], color='purple', bins=30).set_title(f'Transformed: {example_log}')\n",
    "    plt.show()"
   ],
   "id": "f5ca022334ef892f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Applying a Log transformation is aggressive. While it fixes Right-Skewness (positive skew), it risks **\"Overshooting\"**: compressing the data so much that it creates a massive Left-Skew (negative skew).\n",
    "\n",
    "**Verification Criterion:**\n",
    "We compare the absolute skewness before and after transformation ($|Skew_{new}| < |Skew_{old}|$).\n",
    "* **Improved:** The distribution is closer to a Normal shape (0 skew).\n",
    "* **Overshot:** The transformation was too strong, inverting the skew excessively.\n",
    "    * *Note:* In economic variables (like distance), we often tolerate a slight overshoot if it linearizes the relationship with Price, even if the distribution isn't perfectly normal."
   ],
   "id": "4ec3ea7c6c9b4e45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- GROUP B: TRANSFORMATION QUALITY CHECK ---\n",
    "\n",
    "print(f\"{'Variable':<30} | {'Orig Skew':<10} | {'Log Skew':<10} | {'Status'}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# Iterate only through transformed variables\n",
    "transformed_cols = [c for c in df.columns if c.startswith('ln_') and c not in ['ln_price', 'ln_m2_constructed', 'ln_m2_terrain']]\n",
    "\n",
    "for log_col in transformed_cols:\n",
    "    # Reconstruct original column name (remove 'ln_' prefix)\n",
    "    orig_col = f\"feat_{log_col.replace('ln_', '')}\"\n",
    "\n",
    "    if orig_col in df.columns:\n",
    "        orig_skew = df[orig_col].skew()\n",
    "        new_skew = df[log_col].skew()\n",
    "\n",
    "        # Check if absolute skewness was reduced (closer to 0)\n",
    "        # We use abs() because -1.5 is just as bad as +1.5\n",
    "        improved = abs(new_skew) < abs(orig_skew)\n",
    "\n",
    "        status = \"Improved\" if improved else \"Overshot **\"\n",
    "\n",
    "        print(f\"{orig_col:<30} | {orig_skew:>9.2f}  | {new_skew:>9.2f}  | {status}\")"
   ],
   "id": "bb14806e09304462",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Our initial analysis revealed that the **Log transformation** was too aggressive for certain variables (e.g., `dist_mall`, `dist_industrial`), flipping a moderate positive skew into a strong negative skew (\"Overshooting\").\n",
    "\n",
    "**Decision:**\n",
    "Instead of testing an exhaustive list of functions, we apply a pragmatic **Binary Choice Strategy**:\n",
    "1.  **Log ($ln(x+1)$):** Best for heavy tails and exponential decay.\n",
    "2.  **Square Root ($\\sqrt{x}$):** A milder transformation, ideal for moderate skewness where Log is too strong.\n",
    "\n",
    "We will calculate the skewness for both and select the one that brings the distribution closest to zero."
   ],
   "id": "c0fd76f6180d54ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- OPTIMIZED GROUP B TRANSFORMATION ---\n",
    "\n",
    "for col in group_b_decay:\n",
    "    skew_orig = df[col].skew()\n",
    "\n",
    "    if skew_orig > 1.0:\n",
    "\n",
    "        log_val = np.log1p(df[col])\n",
    "        sqrt_val = np.sqrt(df[col])\n",
    "\n",
    "        skew_log = log_val.skew()\n",
    "        skew_sqrt = sqrt_val.skew()\n",
    "\n",
    "        # Decision Rule: Choose the one closest to 0\n",
    "        if abs(skew_log) < abs(skew_sqrt):\n",
    "            # Log wins\n",
    "            df[f\"ln_{col.replace('feat_', '')}\"] = log_val\n",
    "            method = \"Log\"\n",
    "            final_skew = skew_log\n",
    "        else:\n",
    "            # Sqrt wins (Note: we still prefix 'ln_' or change to 'sqrt_' to keep track)\n",
    "            df[f\"sqrt_{col.replace('feat_', '')}\"] = sqrt_val\n",
    "            method = \"Sqrt\"\n",
    "            final_skew = skew_sqrt\n",
    "\n",
    "        print(f\"Variable: {col:<25} | Orig: {skew_orig:.2f} -> New: {final_skew:.2f} ({method})\")"
   ],
   "id": "1a64b98406287c46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We achieved skewness values between **$\\pm 0.6$** for all variables using simple transformations (Log & Sqrt). While algorithms like `PowerTransformer` (Yeo-Johnson) could theoretically push skewness closer to exactly 0, we deliberately stop here for three reasons:\n",
    "\n",
    "1.  **Explainability:** Log and Sqrt have clear economic interpretations (diminishing marginal utility). Complex fractional powers (e.g., $x^{-0.32}$) are \"black boxes\" to stakeholders.\n",
    "2.  **Diminishing Returns:** For a Linear Regression model, the current distribution shapes are sufficient to satisfy linearity assumptions. Further optimization yields negligible predictive gains.\n",
    "3.  **Robustness:** Simpler transformations are less prone to overfitting and easier to maintain in a production pipeline than data-dependent lambdas.\n",
    "\n",
    "We consider \"Decay Factors\" (Group B) to be now statistically healthy and ready for modeling."
   ],
   "id": "472a67df0ecf9778"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "example_col = 'feat_dist_mall'\n",
    "example_log = f\"ln_{example_col.replace('feat_', '')}\"\n",
    "example_sqrt = f\"sqrt_{example_col.replace('feat_', '')}\"\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "sns.histplot(df[example_col], ax=ax[0], color='gray', bins=30).set_title(f'Original: {example_col}')\n",
    "sns.histplot(df[example_log], ax=ax[1], color='purple', bins=30).set_title(f'Log: {example_log}')\n",
    "sns.histplot(df[example_sqrt], ax=ax[2], color='purple', bins=30).set_title(f'Sqrt: {example_sqrt}')\n",
    "plt.show()"
   ],
   "id": "4ccc72615270ee6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 5.2.3. Group C Analysis: Discrete Counters\n",
    "These variables (Bedrooms, Bathrooms, Parking) were capped and cleaned in **Stage A**.\n",
    "Here, we visualize their distribution to confirm they are well-behaved and ready for modeling. No further filtering is required."
   ],
   "id": "5a6e27c6a2013611"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- GROUP C: DISTRIBUTION CHECK ---\n",
    "\n",
    "# No filtering here. Just confirmation.\n",
    "print(f\"--- GROUP C SUMMARY (Post-Stage A Capping) ---\")\n",
    "print(df[group_c_counters].describe().loc[['min', 'max', 'mean']])\n",
    "\n",
    "# Visual Check using Countplots (better for discrete data than histograms)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot first 3 counters as sanity check\n",
    "for i, col in enumerate(group_c_counters[:3]):\n",
    "    if i < 3:\n",
    "        sns.countplot(x=df[col], ax=axes[i], palette='viridis')\n",
    "        axes[i].set_title(f'{col} Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "99bd3444f96d07ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 5.2.4. Group D Analysis: Boolean & Categorical Variance\n",
    "This group includes binary amenities (Pool, Gym) and the One-Hot Encoded municipalities from Stage A.\n",
    "\n",
    "**Strategy:**\n",
    "* **Low Variance Filter:** If a feature is present/absent in more than **99.5%** of the data, it lacks sufficient variance to be predictive and is removed to reduce noise."
   ],
   "id": "e4c505cda4e8c60a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- GROUP D: LOW VARIANCE FILTER ---\n",
    "\n",
    "# Threshold: 99.5% constant (e.g., if only 0.5% of houses have a \"Tennis Court\")\n",
    "variance_threshold = 0.995\n",
    "cols_to_drop = []\n",
    "\n",
    "print(f\"Checking {len(group_d_booleans)} boolean variables for low variance...\")\n",
    "\n",
    "for col in group_d_booleans:\n",
    "    # Calculate frequency of the most common value\n",
    "    top_freq = df[col].value_counts(normalize=True).iloc[0]\n",
    "\n",
    "    if top_freq > variance_threshold:\n",
    "        cols_to_drop.append((col, top_freq))\n",
    "\n",
    "# Execution\n",
    "if cols_to_drop:\n",
    "    print(f\"\\nDROPPING {len(cols_to_drop)} columns (Constant > {variance_threshold*100}%):\")\n",
    "    for col, freq in cols_to_drop:\n",
    "        print(f\"   - {col}: {freq:.2%} constant\")\n",
    "\n",
    "    # Drop from dataframe\n",
    "    drop_list = [x[0] for x in cols_to_drop]\n",
    "    df.drop(columns=drop_list, inplace=True)\n",
    "else:\n",
    "    print(\"\\nAll boolean variables have sufficient variance.\")\n",
    "\n",
    "print(f\"\\n--- FINAL DATASET SHAPE: {df.shape} ---\")"
   ],
   "id": "eb56e77826ec588",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Diagnosing Multicollinearity (The \"Bedroom\" Issue)\n",
    "\n",
    "To confirm why the bedroom coefficient is negative, we need to check how strongly the physical variables relate to each other.\n",
    "\n",
    "**Tools used:**\n",
    "1.  **Correlation Matrix:** Visual inspection of relationships.\n",
    "2.  **VIF (Variance Inflation Factor):** A statistical measure quantifying how much the variance of a coefficient is inflated due to multicollinearity.\n",
    "    * *Rule of Thumb:* VIF > 5 indicates high multicollinearity. VIF > 10 requires immediate action (dropping variables)."
   ],
   "id": "f191994ea7dd1358"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- A. Correlation Matrix Heatmap ---\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df[['target_price', 'feat_bedrooms', 'feat_bathrooms',\n",
    "                  'feat_m2_constructed', 'feat_parking_spots']].corr()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool)) # Hide upper triangle\n",
    "\n",
    "sns.heatmap(corr_matrix,\n",
    "            mask=mask,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cmap='RdBu_r',\n",
    "            vmax=1,\n",
    "            vmin=-1,\n",
    "            linewidths=.5)\n",
    "\n",
    "plt.title('Correlation: Physical Features vs Price', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# --- B. Variance Inflation Factor (VIF) Calculation ---\n",
    "# Select only numeric features for VIF check (excluding target)\n",
    "features_for_vif = df[['feat_bedrooms', 'feat_bathrooms',\n",
    "                       'feat_m2_constructed', 'feat_parking_spots']].dropna()\n",
    "\n",
    "# Create VIF dataframe\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = features_for_vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(features_for_vif.values, i)\n",
    "                   for i in range(len(features_for_vif.columns))]\n",
    "\n",
    "print(\"\\n--- Variance Inflation Factor (VIF) Results ---\")\n",
    "print(vif_data.sort_values(by=\"VIF\", ascending=False))"
   ],
   "id": "8728ebbdf29f645f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
